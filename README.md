# Subreddit Sentiment Classification

Reddit is made of threads which contain posts generated by the users. Main aim in this task is to predict the sentiment polarity of each post individually. The dataset contains target column called “sentiment.polarity” which can take 5 values: "very negative", "negative", "neutral", "positive" and "very positive" (multi-label classification/prediction).

# Code and Resources Used

* Python Version: 3.9
* Packages: pandas, numpy, sklearn, matplotlib, seaborn, nltk, spacy
* Data: Provided by The University of Glasgow as part of coursework.

## Dataset Exploration

This dataset contains Reddit threads, which contain posts generated by users and replies from other users.
The dataset is split into training, validation, and test sets. Each of these contains 12 columns/features
including the target feature (Sentiment Polarity). The training dataset contains 12138 records, while the
validation and test dataset contains 3109 and 4016 records respectively.

![distribution of labels](https://github.com/ArunGautham-Soundarrajan/subreddit_sentiment_classification/blob/main/images/distribution_updated.png)

From the above plot, the dataset is dominated by the number of neutral comments, followed by positive,
negative, and so on. While the Very negative comments being the lowest in all the datasets. This is
imbalanced and would result in misclassifications of minority classes.

## Classifier's Performance

Classifier Name | Accuracy | Macro F1 score(avg) | Weighted F1 score(avg)
--------------- | -------- | ------------------- | ----------------------
Dummy Classifier ( Most Frequent ) | 62.2 | 15.4 | 77
Dummy Classifier ( Stratified ) | 47.2 | 20 | 47.4
Logistic Regression - One hot vectorization | 74.7 | 47.6 | 76.2
Logistic Regression - TF-IDF vectorization | 74.1 | 35.6 | 78
SVC Classifier - One hot vectorization | 73 | 28.7 | 78.2
Decision Tree - TF-IDF vectorization (my choice) | 69.5 | 49 | 69.6

*Note: The input of every classifier has been tokenized and normalized. Vectorizing has been done
individually based on the given requirements for each classifier.*

* **Dummy Classifier with Most frequent**
  
  Dummy classifiers are used to set a baseline performance using simple rules, which can
be used to compare with other real models. Here the strategy is most frequent which predicts the
most frequent class for all the inputs. It got an accuracy of 62.2 percent predicting ‘neutral’ for all
the inputs. This classifier recorded the worst macro f1 score among the other classifiers.

* **Dummy Classifier with Stratified**
  
  This classifier predicts the labels with respect to the distribution of the data in the training
set. This classifier recorded the lowest accuracy of 47.2 percent and the second-lowest macro f1
score with 20 percent. These two classifiers can be set as a baseline to compare the performances
of real classifiers.

* **Logistic Regression - One hot vectorization**

  The Logistic Regression with one hot vectorization has been the best classifier with
respect to the macro f1 score with 47.6 percent. It also tops the table with the highest accuracy
recorded with 74.7 percent.

* **Logistic Regression - TF-IDF vectorization**

  The Logistic Regression with TF-IDF vectorization also performed very well and is just
behind the same model with one hot vectorization. The accuracy of this model is 74.1 percent and
the macro f1 score is 35.6 percent.

* **SVC Classifier - One hot vectorization**

  Looking at the metrics, it is clear that the SVC classifier couldn’t define a proper decision
boundary for each category. But still, it achieved an accuracy of 73 percent and a macro f1 score
of 28.7. Comparing other models. It has the highest weighted f1 score of 78.2

* **Decision Tree Classifier - TF-IDF vectorization**

  Finally Decision tree classifier. I believed this would perform well because it can
make a better decision with categorical features like these. It got the highest average macro f1
score of 49 percent, while the accuracy is still low compared to the other models with 69.5
percent. Hyperparameter tuning could have improved the score, but that’s not the task here.

# Parameter Optimisation

**Classifier:** Logistic Regression with TF-IDF vectorization.

Parameter tuning is used to find the best parameters that work for the particular dataset. There are
several approaches to find the optimal parameters. I have used GridSearchCV to find the optimal
parameters. I used training data and used 4 fold cross-validation to evaluate the model’s parameter on
unseen data. The results of the baseline model and model with tuned parameters are shown below,

Metrics | Baseline Model | Tuned Model
------- | -------------- | -----------
Accuracy | 74.1 | 77.4
Macro-averaged Precision | 33.1 | 46.3
Macro-averaged Recall | 57.4 | 69.9
Macro-averaged F1 | 35.6 | 51.8

From the table, it is clear that the tuned model is better in every aspect. The accuracy increased by
approximately 1 percent, precision by 32 percent, recall by 20 percent, and f1 score by 31 percent. These
are huge improvements.

# Error Analysis

I did error analysis by manually looking at the correct class, predicted class, and the body to
analyze the trends in prediction and to find errors. By generating the world clouds for each class in two
types, one word cloud to find the words that occurred mostly in the correctly predicted class and one to
find the words that occurred only in the mispredicted class. Using this strategy we can find the impact of
words for correct and incorrect predictions.

## Findings

* Emojis like :) , :/ , :< were found during the analysis, which I removed while tokenizing because
it was considered as punctuations. But they would have been a game-changer, as most of the
emojis directly reflect the target class. Next time when preprocessing, we should use regular
expressions to find potential emojis and assign them a word that should be unique and that can be
used as the main feature.
* Most of the negative comments have been classified as neutral, it has been misclassified twice the
amount of correct classification, while some of the negative contents are hard to classify. For
example, ‘ When homework was just cutting and gluing’ doesn’t contain any negative words and
it is hard to classify for humans too.
* Classifying the positive and very positive is very tricky and neutral comes there to add further
confusion. Looking at the wordcloud for misclassified classes instead of classifying them as
positive, doesn’t contain many words that describe positivity, this also could be one of the reasons
for its misclassification.
* On analyzing the neutral class, only 10 percent of them have been misclassified in total. And the
word cloud has a majority of the words, that doesn’t imply any emotions
* Finally, limiting the number of output classes to three (Negative, Neutral, and Positive) would
further improve the performance

# Additional Features

So far we only used the body of the post to classify, now we can add additional features and examine if
that could further improve the model. Looking at the available features, I thought *Majority type* and
*Sentiment subjectivity* can influence the class.

* As the *Majority type* is a categorical feature, we need to *one-hot encode* it.
* Similarly *‘Sentiment subjectivity’* is a numerical value between 0 and 1. So we can use a
min-max scaler or standard scaler to normalize it. I went with the *standard scaler*.

Just by adding these two features, the performance bumped. I also did *feature selection* to select the most
important features, which I thought will further improve the performance. Feature selection I methods
tried,
1. Chi-Squared test
1. L1 regularization
1. Random Forest Feature importance

The chi-squared test gave the best result with *800 features*. We went from about 3250 features to now just
800 features which outperforms that model. The below table shows the performance of three states, tuned
model, model after additional features, and finally after feature selection.

Metrics | Tuned Model | Model after adding features | Model after feature selection
--- | ---- | ---- | ----
Accuracy | 77.4 | 80.6 | 81.8
Macro-averaged Precision | 46.3 | 56 | 59.1
Macro-averaged Recall | 69.9 | 66.9 | 73.1
Macro-averaged F1 | 51.8 | 60.3 | 64.4

I have also tried *Vader sentiment Analyser* to add few more features. I thought that would correlate well
with the target. But to my surprise, that didn’t influence the performance of the model. 

![comparison](https://github.com/ArunGautham-Soundarrajan/subreddit_sentiment_classification/blob/main/images/bs_vs_final.png)

*Comparison of the baseline vs final model*
